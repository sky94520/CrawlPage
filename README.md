# 专利页面[非详情页]爬虫
>针对知网专利，本代码共有两部分组成:
>run_page.py 负责启动爬虫，还负责向下找到一个分支，然后交给scrapy进行爬取
>另外一部分则是scrapy爬虫

>使用到.env，用于区别测试环境和生产环境，二者分开，以便于调试。
>爬取规则放入配置文件中，要爬取的类别放入redis之中
>cookie负责查询的条件，同一类，cookie是相同的，只不过在出现验证码的时候，
>统一进行切换即可。
>现在有两种方法：
>1. 每次只爬取一类，爬取完毕后程序退出。能完整地保证这一类的日志等，也方便管理。
>>每次只爬取一类的话，就需要外部有一个启动器，用来启动这个爬虫，这个不着急。
>先把大致的框架做好
>2. ~~while 从redis拿取所有的，不利于断点重开。~~

## 思路
>run_page.py会根据redis中剩余的分类号，从中取出一个类别进行爬取，此时的redis作为一个
>队列使用。在确认分类号后，在初始时，启动器会检测redis是否存在一个字典
>process {"main_cls": "A", "page": 1, "total_count": 0, "cur_count": 0}
>以此来确定从哪启动，上面的为默认值。在每次抓取一个页面成功的时候,都会更新page的值。
>抓取器在每次抓取页面时，会同时保存页面和解析页面，它也会判断当前的页面个数。
>简而言之，run_page.py会先确认redis中的process是否已经爬取完成，如果是的话，
>才会从redis队列中取出一个类别，并交给scrapy进行爬取
## 文件命名规范
>列表页命名 文件夹为主分类号main_cls_number，文件名为页码(按照年份爬取的页面递增)
## 调度器
>1. 获取process的值，如果没有，则赋予默认值;
>2. 根据值，生成链接，
>3. 根据链接爬取页面
>4. 按照code为文件夹名和页码为文件名保存爬取到的页面
>5. 对爬取到的页面解析，获取总个数，并解析链接，并把得到的链接放入【】中，之后更新process的进度值。
>6. 判断当前解析的个数是否等于总个数，如果等于，则表示这个类别
>爬取完成，则接着向下个类别爬取；否则继续从步骤1开始。

## 断点
>程序可以断点调试
## 问题
> 1. 当某一类别的数据超过6000个时，则只能获取6000个数据，当前的办法是按照公开日再次进行
>拆分，在cookie中添加date_gkr_from date_gkr_to两个字段。
>目前就出现两种，第一种是小于6000个值的，此时不需要拆分，直接进行即可。
>第二种是大于6000的则按照天数拆分，默认的天数为366天（不超过一年），当超过6000时，
>先尝试使用一年（根据timedelta(days=366)），之后days递减
> 2. 进行统计 统计各个文件夹的所有链接个数，去重之后的个数，按照树型结构合并的个数。

## 所需外部环境
>### 1. redis
>本爬虫使用了redis作为队列使用，爬虫启动时，会先判断redis中是否存在断点，
>如果存在则继续爬取，否则从队列中弹出一个元素，并继续爬取。

##当要爬取的页面的条目个数大于阈值时的处理
> 首先是要先获取一次页面才能知道在这个分类下的个数，然后再尝试按照当前的日期并且减去
>365天来获取这约一年的个数（保证按照年份，便于调试），如果仍然大于阈值，则依次
>除以2.

## middleware
> 设置代理，会发送请求requests获取代理
> 设置cookie，会检测spider之前的cookie是否已经不可用，如果不可用，则重新发起请求获取
> 重写最大值重试次数中间件 功能只是添加了一个日志输出
## pipeline
> JsonPipeline 用于把解析出来的数据保存到json文件中
> SavePagePipeline 保存原始的页面
